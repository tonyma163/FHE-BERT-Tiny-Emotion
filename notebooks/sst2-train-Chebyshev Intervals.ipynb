{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "591beb8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tonyma/code/FHE-BERT-Tiny-Emotion/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt \n",
    "from datasets import load_dataset\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bff96f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at prajjwal1/bert-tiny and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#The same as https://huggingface.co/google/bert_uncased_L-2_H-128_A-2\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"prajjwal1/bert-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a998ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(correct, approx):\n",
    "    if type(approx) == list:\n",
    "        approx = np.array(approx)\n",
    "    absolute = sum(abs(correct - approx))/len(correct)\n",
    "    relative = absolute / (sum(abs(correct))/len(correct))\n",
    "    return 1 - relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d2018e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3t/qv8zrd4d5t12dqxjq4rxf56r0000gn/T/ipykernel_20669/3694534078.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  trained = torch.load('SST-2-BERT-tiny.bin', map_location=torch.device('cpu'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 128)\n",
       "      (token_type_embeddings): Embedding(2, 128)\n",
       "      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-1): 2 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "              (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "            (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained = torch.load('SST-2-BERT-tiny.bin', map_location=torch.device('cpu'))\n",
    "trained.pop('bert.embeddings.position_ids', None) # Remove unexpected keys\n",
    "model.load_state_dict(trained , strict=True)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f98925ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_contribuisci(x, index = 1):\n",
    "    if index == 1:\n",
    "        input_exp_1.append(torch.max(x).item())\n",
    "        input_exp_1.append(torch.min(x).item())\n",
    "    else:\n",
    "        input_exp_2.append(torch.max(x).item())\n",
    "        input_exp_2.append(torch.min(x).item())\n",
    "    \n",
    "    for head in x.squeeze():\n",
    "        for row in head:\n",
    "            if index == 1:\n",
    "                input_inv_1.append(torch.sum(torch.exp(row)).item())\n",
    "            else:\n",
    "                input_inv_2.append(torch.sum(torch.exp(row)).item())\n",
    "                \n",
    "    return torch.softmax(x, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea94b047",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = pd.read_parquet(\"SST-2-val.parquet\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"stanfordnlp/sst2\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92ec3e5",
   "metadata": {},
   "source": [
    "Since the training dataset is too large, splitting into 10 parts for inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c05141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = int(len(dataset) / 10)  # split into 10 parts\n",
    "\n",
    "train_dataset1 = dataset.select(range(0, subset_size))\n",
    "train_dataset2 = dataset.select(range(subset_size, subset_size * 2))\n",
    "train_dataset3 = dataset.select(range(subset_size * 2, subset_size * 3))\n",
    "train_dataset4 = dataset.select(range(subset_size * 3, subset_size * 4))\n",
    "train_dataset5 = dataset.select(range(subset_size * 4, subset_size * 5))\n",
    "train_dataset6 = dataset.select(range(subset_size * 5, subset_size * 6))\n",
    "train_dataset7 = dataset.select(range(subset_size * 6, subset_size * 7))\n",
    "train_dataset8 = dataset.select(range(subset_size * 7, subset_size * 8))\n",
    "train_dataset9 = dataset.select(range(subset_size * 8, subset_size * 9))\n",
    "train_dataset10 = dataset.select(range(subset_size * 9, len(dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7726791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6734\n",
      "6734\n",
      "6734\n",
      "6734\n",
      "6734\n",
      "6734\n",
      "6734\n",
      "6734\n",
      "6734\n",
      "6743\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset1))\n",
    "print(len(train_dataset2))\n",
    "print(len(train_dataset3))\n",
    "print(len(train_dataset4))\n",
    "print(len(train_dataset5))\n",
    "print(len(train_dataset6))\n",
    "print(len(train_dataset7))\n",
    "print(len(train_dataset8))\n",
    "print(len(train_dataset9))\n",
    "print(len(train_dataset10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66163b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [08:45<00:00, 12.82it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "input_exp_1 = []\n",
    "input_inv_1 = []\n",
    "input_gelu_1 = np.array([])\n",
    "input_exp_2 = []\n",
    "input_inv_2 = []\n",
    "input_gelu_2 = np.array([])\n",
    "input_tanh = np.array([])\n",
    "\n",
    "fhe_correct = 0\n",
    "fhe_wrong = 0\n",
    "\n",
    "fhe_accuracy = 0\n",
    "std_accuracy = 0\n",
    "\n",
    "for ind in tqdm(range(len(train_dataset1))):\n",
    "    text = \"[CLS] \" + train_dataset1['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset1['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset1['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset1['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset1['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e90a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [22:32<00:00,  4.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# training dataset 2\n",
    "for ind in tqdm(range(len(train_dataset2))):\n",
    "    text = \"[CLS] \" + train_dataset2['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset2['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset2['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset2['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset2['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a0f806e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [34:22<00:00,  3.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# training dataset 3\n",
    "for ind in tqdm(range(len(train_dataset3))):\n",
    "    text = \"[CLS] \" + train_dataset3['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset3['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset3['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset3['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset3['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "407268e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [51:27<00:00,  2.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# training dataset 4\n",
    "for ind in tqdm(range(len(train_dataset4))):\n",
    "    text = \"[CLS] \" + train_dataset4['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset4['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset4['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset4['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset4['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9575474",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [1:16:50<00:00,  1.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# training dataset 5\n",
    "for ind in tqdm(range(len(train_dataset5))):\n",
    "    text = \"[CLS] \" + train_dataset5['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset5['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset5['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset5['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset5['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27246d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [1:37:15<00:00,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# training dataset 6\n",
    "for ind in tqdm(range(len(train_dataset6))):\n",
    "    text = \"[CLS] \" + train_dataset6['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset6['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset6['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset6['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset6['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33c8c696",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [1:44:39<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# training dataset 7\n",
    "for ind in tqdm(range(len(train_dataset7))):\n",
    "    text = \"[CLS] \" + train_dataset7['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset7['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset7['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset7['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset7['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a08396fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [2:03:45<00:00,  1.10s/it]  \n"
     ]
    }
   ],
   "source": [
    "# training dataset 8\n",
    "for ind in tqdm(range(len(train_dataset8))):\n",
    "    text = \"[CLS] \" + train_dataset8['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset8['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset8['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset8['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset8['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bb6b4b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6734/6734 [2:21:00<00:00,  1.26s/it]  \n"
     ]
    }
   ],
   "source": [
    "# training dataset 9\n",
    "for ind in tqdm(range(len(train_dataset9))):\n",
    "    text = \"[CLS] \" + train_dataset9['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset9['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset9['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset9['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset9['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9f23cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6743/6743 [2:44:58<00:00,  1.47s/it]  \n"
     ]
    }
   ],
   "source": [
    "# training dataset 10\n",
    "for ind in tqdm(range(len(train_dataset10))):\n",
    "    text = \"[CLS] \" + train_dataset10['sentence'][ind] + \" [SEP]\"\n",
    "\n",
    "    tokenized = tokenizer(text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "        \n",
    "    x = model.bert.embeddings(tokens_tensor, torch.tensor([[1] * len(tokenized_text)]))\n",
    "\n",
    "    key = model.bert.encoder.layer[0].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[0].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[0].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[0].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[0].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[0].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = x.double()\n",
    "\n",
    "    input_tensor = x.double()\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([-0.03383045433490704, -0.04689138747464171, -0.04320052751297194, -0.04194874763842685, -0.03849735236740709, -0.03583471496309556, -0.036673685450259945, -0.03533623114666153, -0.03301200050649906, -0.03385619903604035, -0.03394064677150061, -0.03581378040060232, -0.04000193681582013, -0.042994980738727644, -0.042689484809151766, -0.0422699887342667, -0.040702211423783496, -0.043257636922742766, -0.040924377288572664, -0.04212762593354266, -0.040090620729304687, -0.03727317047412721, -0.030603299343800818, -0.034141189654495016, -0.03468711091296442, -0.032307857857310274, -0.02926372943560165, -0.031292906450152466, -0.037837883896213766, -0.03745859562807607, -0.03794657692710982, -0.03860214509229593, -0.036185650111238955, -0.039154371235979875, -0.03589729976884486, -0.031731895884233016, -0.03465287223481833, -0.031348414682812194, -0.03688161652969029, -0.03338290816163936, -0.038240660222183975, -0.037525466450406116, -0.038229222217722264, -0.041201914113547705, -0.04212576296359885, -0.03980083151775188, -0.04072657806877826, -0.040145599490268025, -0.036685242667777444, -0.034109016054392725, -0.03544325775104831, -0.03623692053970561, -0.04948334692050963, -0.04596823422981405, -0.04892271117435003,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.7495962428549272, 0.6109555428467895, 0.6225590467577651, 0.62495153067201, 0.631395549935461, 0.634492711694546, 0.644892789064359, 0.6542099965205022, 0.6595559062153842, 0.6659906881037033, 0.6680168012366937, 0.6758412527257586, 0.6668118068796066, 0.6718192460326265, 0.67786737736941, 0.6808577853930836, 0.6736657333151266, 0.6676446046843724, 0.6659979061989304, 0.6743226078654423, 0.681388263935704, 0.6837117808950258, 0.6907147768934253, 0.684537831509984, 0.6896744328697597, 0.6916627127801457, 0.6954043965468235, 0.6954046755145293, 0.7001025287354249, 0.695094327647078, 0.6854203403085795, 0.7027792682295838, 0.6956849098218769, 0.6945153573872891, 0.6856697060013522, 0.6897353511373785, 0.700668908202082, 0.6965624918742969, 0.7082690699456209, 0.7043163331126293, 0.7070770512949652, 0.7042510307314358, 0.6978925459183357, 0.7205035876616076, 0.6902461198740245, 0.686971254827903, 0.7028843270104062, 0.7032880792671149, 0.7057843340136714, 0.7104860015626775, 0.7321738164781159, 0.71095817492914, 0.7401485084476891, 0.7312957890728539, 0.7375994654874705,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[0].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[0].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[0].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_1 = np.append(input_gelu_1, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[0].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[0].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    mean = np.array([-0.09545516102868973, 0.034540955180462664, 0.03934738149667437, 0.040802318439555035, 0.04426037798445811, 0.04919343175846099, 0.0493616301294401, 0.047896279398118795, 0.04912640635535303, 0.048717249992826256, 0.0477219385203478, 0.05095357678578503, 0.05094908370417657, 0.0493275745992752, 0.048418324664654545, 0.0473653504669205, 0.04528009986283869, 0.04524247257539856, 0.046555073355952846, 0.0516135997743503, 0.049103903254210594, 0.048877585502238356, 0.048364988370661784, 0.049043507301742846, 0.049933470462367846, 0.05175179126331398, 0.05057227793143223, 0.055763206569478994, 0.055243365455213404, 0.04986745821758072, 0.047789218698650125, 0.047852162700887234, 0.04279460740337753, 0.04280733225675328, 0.04644169155736491, 0.04783492130826333, 0.04759649093761958, 0.045252139153821, 0.04367184005341422, 0.039034762655413016, 0.04374965234639466, 0.04355128435775863, 0.04499861862695065, 0.04318602336450084, 0.04549296197766528, 0.03907804279518851, 0.037683132925437485, 0.04109696491189214, 0.04410155617431274, 0.05015992918511731, 0.04335430986396108, 0.046492484403760526, 0.044277581701870204, 0.03723061917091777, 0.039156973130334664,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0 \n",
    "                    ])\n",
    "    var = np.array([0.4156698594967092, 0.7008452266859936, 0.7214270983257646, 0.7095727482866087, 0.7102521835201318, 0.710293676073547, 0.7091783271698753, 0.6973493176419543, 0.7011688527520855, 0.7007704875343309, 0.6950537183089973, 0.6948029158092094, 0.6919309911197036, 0.6933694537037308, 0.6970711644923971, 0.7004276850010867, 0.6964234913676165, 0.6987678419874651, 0.6951829293138483, 0.6973048809142951, 0.6989420799277399, 0.7005696487948311, 0.6993937733493811, 0.6902070532566239, 0.6958399824203775, 0.6900361005407983, 0.6925891359742274, 0.6831642926666377, 0.6865279710039072, 0.6904370385593245, 0.6963724536275457, 0.6948942601360332, 0.6784634186071326, 0.6759657478656234, 0.6828578884489792, 0.683566347862741, 0.6857777074044566, 0.672040915409448, 0.6784995422914343, 0.6732453264186854, 0.683881765911935, 0.6909411690410042, 0.6715428435769978, 0.6775867807314924, 0.6785015863916147, 0.676156117696202, 0.6786376609996214, 0.6763771062984715, 0.7119440584663215, 0.7070342067744777, 0.6895996022331654, 0.6683970656272868, 0.6695013664908844, 0.6566575067124804, 0.672887703816164,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    fin7_whole = []\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[0].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "    \n",
    "    real = model.bert.encoder.layer[0](x)[0].transpose(1, 2).reshape(-1).detach()\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, fin7_whole[0].transpose(0, 1).reshape(-1).detach())))\n",
    "    \n",
    "    key = model.bert.encoder.layer[1].attention.self.key.weight.clone().detach().double().transpose(0, 1)\n",
    "    query = model.bert.encoder.layer[1].attention.self.query.weight.clone().detach().double().transpose(0, 1)\n",
    "    value = model.bert.encoder.layer[1].attention.self.value.weight.clone().detach().double().transpose(0, 1)\n",
    "\n",
    "    key_bias = model.bert.encoder.layer[1].attention.self.key.bias.clone().detach().double()\n",
    "    query_bias = model.bert.encoder.layer[1].attention.self.query.bias.clone().detach().double()\n",
    "    value_bias = model.bert.encoder.layer[1].attention.self.value.bias.clone().detach().double()\n",
    "\n",
    "    original_input_tensor = fin7_whole\n",
    "    input_tensor = fin7_whole\n",
    "\n",
    "    q = torch.matmul(input_tensor, query) + query_bias\n",
    "    k = torch.matmul(input_tensor, key) + key_bias\n",
    "    v = torch.matmul(input_tensor, value) + value_bias\n",
    "\n",
    "    q = q.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    k = k.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "    v = v.reshape([1, input_tensor.size()[1], 2, 64])\n",
    "\n",
    "    q = q.permute([0, 2, 1, 3])\n",
    "    k = k.permute([0, 2, 3, 1])\n",
    "\n",
    "    qk = torch.matmul(q, k)\n",
    "    qk = qk / 8\n",
    "\n",
    "    qk_softmaxed = softmax_contribuisci(qk, 2)\n",
    "\n",
    "    v = v.permute([0, 2, 1, 3])\n",
    "\n",
    "    fin = torch.matmul(qk_softmaxed, v)\n",
    "    fin = fin.permute([0, 2, 1, 3])\n",
    "    fin = fin.reshape([1, input_tensor.size()[1], 128])\n",
    "    \n",
    "    mean = np.array([0.04805131047475803, 0.014145706172069285, 0.010630181813540026, 0.010521146572975027, 0.00956244983947186, 0.008211288558782809, 0.008817800275674387, 0.008911457532306733, 0.008643898058317862, 0.008801769546523253, 0.009472254700839258, 0.008094415948174241, 0.007702615754430344, 0.005460620353838359, 0.007021847370084451, 0.008373831982472147, 0.01022061224155272, 0.00927594903773269, 0.009277225000069925, 0.007049453120897054, 0.008682554190420182, 0.008749022040809715, 0.010118317324741522, 0.008998865743435887, 0.008763833543884292, 0.008285728555981435, 0.006967351876718886, 0.00588068616144895, 0.0030701809065725363, 0.003659716972971551, 0.002116778487431024, 0.003947434346765913, 0.006907859825079262, 0.008494112860837831, 0.007040283968419036, 0.007197681884381672, 0.008232685835987293, 0.009965029801574864, 0.00731962961637719, 0.00830555309310382, 0.005340440177451385, 0.007833324368720607, 0.01047456825511633, 0.009674864773662995, 0.010093537461664302, 0.01588798917017868, 0.018537933333636507, 0.018245848282989877, 0.012253993810893607, 0.011354133953173591, 0.013474744814287221, 0.013707011955501919, 0.007918842609048385, 0.017240907760895086, 0.03465881962238184,            \n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    var = np.array([0.6741653046411179, 0.602392389437227, 0.5945841451997256, 0.5997135932136959, 0.6033806506910513, 0.6064839949503851, 0.6058735285405447, 0.6059001754921257, 0.6086086189801689, 0.6118981975241923, 0.6161533101614306, 0.6105411757987637, 0.6102443339235957, 0.6004337682468068, 0.6068584434133084, 0.6123178593290803, 0.6150302868629213, 0.6102744641580546, 0.6143169356654037, 0.6105845722771672, 0.61540315154488, 0.622109065598561, 0.6221720668578823, 0.6279330579960701, 0.6282907135959079, 0.6258439179151315, 0.6187239026398644, 0.618294817104495, 0.609488586748927, 0.6085185174201381, 0.6154275326252285, 0.6207534846328591, 0.6290521066315713, 0.6375810334496135, 0.6238236165346044, 0.6310571465398529, 0.6350551779511981, 0.6452639043477173, 0.6346915398812409, 0.646622546259538, 0.6435498445423712, 0.6401589932559348, 0.6458833892517316, 0.6354378204804867, 0.651796667347259, 0.6547600574517144, 0.6554038815336571, 0.655910889886979, 0.6412602949793637, 0.6489736968517984, 0.6633309254993116, 0.6771441398382873, 0.6423362709438692, 0.6302863730404997, 0.5940213893371686,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0,0,0,0,0,0,0,0,\n",
    "    0,0,0\n",
    "                    ])\n",
    "    \n",
    "    w_output_dense = model.bert.encoder.layer[1].attention.output.dense.weight.clone().detach().double().transpose(0, 1)\n",
    "    b_output_dense = model.bert.encoder.layer[1].attention.output.dense.bias.clone().detach().double()\n",
    "\n",
    "    fin2 = torch.matmul(fin, w_output_dense) + b_output_dense\n",
    "    fin2_backup = fin2.clone()\n",
    "    fin2_backup = fin2_backup + original_input_tensor\n",
    "\n",
    "    fin3_whole = []\n",
    "\n",
    "    for i in range(len(original_input_tensor.squeeze())):\n",
    "        fin2 = fin2_backup.squeeze()[i]\n",
    "\n",
    "        fin3_corr = (fin2.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        #TODO QUA STO USANDO I VERI VALORI!!!!\n",
    "        #fin3_corr = (fin2.squeeze().detach() - torch.mean(fin2.squeeze())) / math.sqrt(torch.var(fin2.squeeze()))\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].attention.output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin3_corr = fin3_corr * w_output_layernorm + b_output_layernorm\n",
    "        fin3_whole.append(fin3_corr.detach())\n",
    "\n",
    "    fin3_whole = torch.cat(tuple(fin3_whole), 0).unsqueeze(0)\n",
    "    fin_4 = torch.matmul(fin3_whole, model.bert.encoder.layer[1].intermediate.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].intermediate.dense.bias\n",
    "\n",
    "    input_gelu_2 = np.append(input_gelu_2, fin_4.reshape(-1).detach().numpy())\n",
    "    \n",
    "    fin_5 = torch.nn.functional.gelu(fin_4)\n",
    "    \n",
    "    fin_6 = torch.matmul(fin_5, model.bert.encoder.layer[1].output.dense.weight.transpose(0, 1).double()) + model.bert.encoder.layer[1].output.dense.bias\n",
    "    fin_6 = fin_6 + fin3_whole\n",
    "    \n",
    "    fin7_whole = []\n",
    "    \n",
    "    mean = np.array([0.06643368, 0.05726708, 0.05311476, 0.05229822, 0.05352628,\n",
    "       0.05238868, 0.0536801 , 0.05327334, 0.05206954, 0.05110339,\n",
    "       0.051747  , 0.05016997, 0.04943122, 0.04937956, 0.04952862,\n",
    "       0.04973959, 0.04852742, 0.04696055, 0.04846476, 0.04925392,\n",
    "       0.0509005 , 0.05373027, 0.05371865, 0.05446217, 0.05222489,\n",
    "       0.05142676, 0.05080909, 0.05179351, 0.05049174, 0.04965748,\n",
    "       0.05138143, 0.0499965 , 0.05194982, 0.05178364, 0.0521023 ,\n",
    "       0.05059624, 0.05445499, 0.05507825, 0.05241935, 0.05073552,\n",
    "       0.05200171, 0.04858642, 0.04419684, 0.04642237, 0.05115073,\n",
    "       0.05028116, 0.05021724, 0.05312114, 0.0524375 , 0.04643478,\n",
    "       0.05026358, 0.04248708, 0.04675281, 0.03895142, 0.04558007,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "    \n",
    "    var = np.array([0.81992316, 0.78486345, 0.79259   , 0.79754392, 0.79350872,\n",
    "       0.79652433, 0.79935746, 0.79867687, 0.80257863, 0.80235328,\n",
    "       0.80521209, 0.80621272, 0.80330435, 0.80469855, 0.81171202,\n",
    "       0.81136354, 0.80977166, 0.8089956 , 0.8106946 , 0.80862825,\n",
    "       0.81450049, 0.81722176, 0.82121488, 0.82012788, 0.8254015 ,\n",
    "       0.82097106, 0.81742119, 0.82090554, 0.82116105, 0.82017896,\n",
    "       0.82234659, 0.82832269, 0.82888739, 0.81852014, 0.82054523,\n",
    "       0.8224114 , 0.82913892, 0.8289046 , 0.81985612, 0.83341215,\n",
    "       0.82896934, 0.82315006, 0.82802216, 0.81886278, 0.8274004 ,\n",
    "       0.83436616, 0.82014282, 0.82628005, 0.83230868, 0.84511334,\n",
    "       0.85141143, 0.84934269, 0.83041272, 0.826798  , 0.83660989,\n",
    "       0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0,0,0,0,0,0,0,0,\n",
    "                    0,0,0])\n",
    "\n",
    "\n",
    "    for i in range(len(input_tensor.squeeze())):\n",
    "        fin_7 = fin_6.squeeze()[i]\n",
    "\n",
    "        fin7_corr = (fin_7.squeeze().detach() - mean[i]) * var[i]\n",
    "\n",
    "        w_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.weight.clone().detach().double().unsqueeze(0)\n",
    "        b_output_layernorm = model.bert.encoder.layer[1].output.LayerNorm.bias.clone().detach().double()\n",
    "\n",
    "        fin7_corr = fin7_corr * w_output_layernorm + b_output_layernorm\n",
    "\n",
    "        fin7_whole.append(fin7_corr.detach())\n",
    "\n",
    "    fin7_whole = torch.cat(tuple(fin7_whole), 0).unsqueeze(0)\n",
    "\n",
    "    real = model.bert.encoder.layer[1](model.bert.encoder.layer[0](x)[0])[0].transpose(1, 2).reshape(-1).detach()\n",
    "    correct = fin7_whole[0].transpose(0, 1).reshape(-1).detach()\n",
    "\n",
    "    input_tanh = np.append(input_tanh, (torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias).reshape(-1).detach())\n",
    "    \n",
    "    #print(\"Precision: {}\".format(precision(real, correct)))\n",
    "    densed_pooler = torch.tanh(torch.matmul(fin7_whole.double(), model.bert.pooler.dense.weight.transpose(0, 1).double()) + model.bert.pooler.dense.bias)\n",
    "\n",
    "    approx = densed_pooler[0][0].detach()\n",
    "    precise = model.bert.pooler(model.bert.encoder(x)[0]).detach()[0]\n",
    "\n",
    "    #print(precision(precise, approx))\n",
    "    \n",
    "    output = torch.matmul(approx, model.classifier.weight.transpose(0, 1).double()) + model.classifier.bias.double()\n",
    "    output_real = model(tokens_tensor, torch.tensor([[1] * len(tokenized_text)])).logits[0].detach()\n",
    "    \n",
    "    if output[0].item() > output[1].item() and output_real[0].item() > output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    elif output[0].item() < output[1].item() and output_real[0].item() < output_real[1].item():\n",
    "        fhe_correct = fhe_correct + 1\n",
    "    else:\n",
    "        fhe_wrong = fhe_wrong + 1\n",
    "        \n",
    "    if output[0].item() > output[1].item() and train_dataset10['label'][ind] == 0:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "    if output[0].item() < output[1].item() and train_dataset10['label'][ind] == 1:\n",
    "        fhe_accuracy = fhe_accuracy + 1\n",
    "        \n",
    "    if output_real[0].item() > output_real[1].item() and train_dataset10['label'][ind] == 0:\n",
    "        std_accuracy = std_accuracy + 1\n",
    "    if output_real[0].item() < output_real[1].item() and train_dataset10['label'][ind] == 1:\n",
    "        std_accuracy = std_accuracy + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22fdfd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard model accuracy:    0.970526659638599\n",
      "Precomputed model accuracy: 0.9623156988225512\n"
     ]
    }
   ],
   "source": [
    "print(\"Standard model accuracy:    {}\\nPrecomputed model accuracy: {}\".format(std_accuracy / len(dataset), fhe_accuracy / len(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa05f2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.338355419275876\n",
      "11546.018829853236\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Encoder 1 - Inverse 1/x\")\n",
    "plt.plot(input_inv_1)\n",
    "print(min(input_inv_1))\n",
    "print(max(input_inv_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5f8f6bd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-14.577587214831471\n",
      "11.541062486215607\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Encoder 1 - GELU\")\n",
    "input_gelu_1 = np.array(input_gelu_1).reshape(-1)\n",
    "#plt.plot(input_gelu_1)\n",
    "print(min(input_gelu_1))\n",
    "print(max(input_gelu_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ee137645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1653476385500318\n",
      "14442144.626600342\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Encoder 2 - Inverse 1/x\")\n",
    "plt.plot(input_inv_2)\n",
    "print(min(input_inv_2))\n",
    "print(max(input_inv_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b985f03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-17.572206871443797\n",
      "22.583263934433408\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Encoder 2 - GELU\")\n",
    "input_gelu_1 = np.array(input_gelu_2).reshape(-1)\n",
    "print(min(input_gelu_2))\n",
    "print(max(input_gelu_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1949fe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-49.74612563409435\n",
      "42.17698394528695\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi4AAAGzCAYAAAAIWpzfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAkEklEQVR4nO3de3CU1cHH8V8S2A0WExDIBuhKuBRQUMAE0oC8DMxKLBRKR0sAhZiK1+gIqRcCSkCUIAUMlSCCXJwONKgV6kgmVKKMoukwAmm1AopcdcxCVBIMkkD2vH84rl2TYDbkwkm+n5n9Yw/n2ecsj7jfefbZ3RBjjBEAAIAFQpt6AQAAALVFuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAqJU77rhDMTExTb2MJjdv3jyFhISouLi4qZcCtEiEC2CZDRs2KCQkxH8LDw9X79699cADD8jr9Tb18hrN0aNHA/4eLnY7evRoUy8XQD1p1dQLAFA3Tz75pLp3765z585p165dev7555Wbm6uPPvpIV1xxRVMvr8F16tRJf/3rXwPGli5dqs8//1zPPvtslbkAmgfCBbDUb37zG8XFxUmSpk+frg4dOmjZsmX6xz/+ocmTJzfx6i7OGKNz586pTZs2dX6MX/ziF7r99tsDxnJycvTNN99UGQfQfPBWEdBMjBo1SpJ05MgRSdKFCxe0YMEC9ezZU06nUzExMZo9e7bKy8urbLty5Ur169dPTqdTXbp0UWpqqk6fPv2z+/T5fMrKylK/fv0UHh4ul8ule+65R998803AvJiYGP32t7/V9u3bFRcXpzZt2uiFF1649CddC0uWLNHQoUPVoUMHtWnTRrGxsXr11VerzAsJCdEDDzygrVu3qn///nI6nerXr5/y8vKqfdzTp0/rjjvuULt27RQZGamUlBSdPXu2oZ8O0OIRLkAz8dlnn0mSOnToIOn7szBz587VDTfcoGeffVYjRoxQZmamJk2aFLDdvHnzlJqaqi5dumjp0qW65ZZb9MILL2j06NE6f/78Rfd5zz336JFHHtGwYcO0fPlypaSkaOPGjUpMTKyy7cGDBzV58mTddNNNWr58uQYOHFh/T/4ili9frkGDBunJJ5/UwoUL1apVK/3hD3/Qtm3bqszdtWuX7r//fk2aNEmLFy/WuXPndMstt+irr76qMnfixIk6c+aMMjMzNXHiRG3YsEHz589vjKcEtGwGgFXWr19vJJkdO3aYU6dOmRMnTpicnBzToUMH06ZNG/P555+bwsJCI8lMnz49YNuHH37YSDJvvfWWMcaYkydPGofDYUaPHm0qKyv981asWGEkmXXr1vnHkpOTTbdu3fz33333XSPJbNy4MWAfeXl5Vca7detmJJm8vLz6/KuoYuzYsQFrNMaYs2fPBtyvqKgw/fv3N6NGjQoYl2QcDoc5dOiQf+zf//63kWSee+45/1hGRoaRZP74xz8GbP/73//edOjQoZ6eCYCacMYFsJTH41GnTp3kdrs1adIktW3bVlu2bFHXrl2Vm5srSUpLSwvY5k9/+pMk+c827NixQxUVFZoxY4ZCQ3/838Fdd92liIiIas9K/OCVV15RZGSkbrrpJhUXF/tvsbGxatu2rd5+++2A+d27d1diYmK9PPdg/O91NN98841KSko0fPhw7d27t8pcj8ejnj17+u9ff/31ioiI0OHDh6vMvffeewPuDx8+XF999ZVKS0vrcfUAfoqLcwFLZWdnq3fv3mrVqpVcLpf69Onjj49jx44pNDRUvXr1CtgmOjpa7dq107Fjx/zzJKlPnz4B8xwOh3r06OH/8+p8+umnKikpUVRUVLV/fvLkyYD73bt3r9Xz+vbbb/Xtt9/674eFhV3Sp4LeeOMNPfXUUyosLAy4vickJKTK3KuvvrrKWPv27atcs1Pd3Pbt20v6Po4iIiLqvF4AF0e4AJYaMmSI/1NFNanuxbm++Hw+RUVFaePGjdX++U9jo7afIFqyZEnAtSLdunWr8/ewvPvuuxo/frz+7//+TytXrlTnzp3VunVrrV+/Xps2baoyPywsrNrHMcZc0lwA9YdwAZqhbt26yefz6dNPP9U111zjH/d6vTp9+rS6devmnyd9f+Fsjx49/PMqKip05MgReTyeGvfRs2dP7dixQ8OGDbukjzX/1LRp03TjjTf671/KY//9739XeHi4tm/fLqfT6R9fv379Ja0RQNPhGhegGRozZowkKSsrK2B82bJlkqSxY8dK+v6aDofDob/85S8BZwrWrl2rkpIS/7zqTJw4UZWVlVqwYEGVP7tw4UKtPk5dnR49esjj8fhvw4YNq9PjSN+fFQkJCVFlZaV/7OjRo9q6dWudHxNA0+KMC9AMDRgwQMnJyVq9erVOnz6tESNGaPfu3XrppZc0YcIEjRw5UtL3b+ekp6dr/vz5uvnmmzV+/HgdPHhQK1eu1ODBgy/6RW4jRozQPffco8zMTBUWFmr06NFq3bq1Pv30U73yyitavny5br311sZ6ytUaO3asli1bpptvvllTpkzRyZMnlZ2drV69euk///lPk64NQN0QLkAz9eKLL6pHjx7asGGDtmzZoujoaKWnpysjIyNg3rx589SpUyetWLFCM2fO1FVXXaW7775bCxcuVOvWrS+6j1WrVik2NlYvvPCCZs+erVatWikmJka33377JZ0pqS+jRo3S2rVrtWjRIs2YMUPdu3fXM888o6NHjxIugKVCDFeSAQAAS3CNCwAAsAbhAgAArEG4AAAAawQdLu+8847GjRunLl26KCQkpFYfK9y5c6duuOEGOZ1O9erVSxs2bKjDUgEAQEsXdLiUlZVpwIABys7OrtX8I0eOaOzYsRo5cqQKCws1Y8YMTZ8+Xdu3bw96sQAAoGW7pE8VhYSEaMuWLZowYUKNcx577DFt27ZNH330kX9s0qRJOn36tPLy8uq6awAA0AI1+Pe4FBQUVPna8MTERM2YMaPGbcrLywN+DM3n8+nrr79Whw4dGvS3VwAAQP0xxujMmTPq0qVLwC/QX4oGD5eioiK5XK6AMZfLpdLSUn333XfV/g5JZmZmwI+sAQAAe504cUK//OUv6+WxLstvzk1PT1daWpr/fklJia6++mqdOHGCn4sHAMASpaWlcrvduvLKK+vtMRs8XKKjo+X1egPGvF6vIiIiavzVV6fTGfBLrj+IiIggXAAAsEx9XubR4N/jkpCQoPz8/ICxN998UwkJCQ29awAA0MwEHS7ffvutCgsLVVhYKOn7jzsXFhbq+PHjkr5/m2fatGn++ffee68OHz6sRx99VAcOHNDKlSv18ssva+bMmfXzDAAAQIsRdLh88MEHGjRokAYNGiRJSktL06BBgzR37lxJ0pdffumPGEnq3r27tm3bpjfffFMDBgzQ0qVL9eKLLyoxMbGengIAAGgprPh16NLSUkVGRqqkpIRrXAAAsERDvH7zW0UAAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxRp3DJzs5WTEyMwsPDFR8fr927d190flZWlvr06aM2bdrI7XZr5syZOnfuXJ0WDAAAWq6gw2Xz5s1KS0tTRkaG9u7dqwEDBigxMVEnT56sdv6mTZs0a9YsZWRkaP/+/Vq7dq02b96s2bNnX/LiAQBAyxJ0uCxbtkx33XWXUlJSdO2112rVqlW64oortG7dumrnv//++xo2bJimTJmimJgYjR49WpMnT/7ZszQAAAA/FVS4VFRUaM+ePfJ4PD8+QGioPB6PCgoKqt1m6NCh2rNnjz9UDh8+rNzcXI0ZM6bG/ZSXl6u0tDTgBgAA0CqYycXFxaqsrJTL5QoYd7lcOnDgQLXbTJkyRcXFxbrxxhtljNGFCxd07733XvStoszMTM2fPz+YpQEAgBagwT9VtHPnTi1cuFArV67U3r179dprr2nbtm1asGBBjdukp6erpKTEfztx4kRDLxMAAFggqDMuHTt2VFhYmLxeb8C41+tVdHR0tds88cQTmjp1qqZPny5Juu6661RWVqa7775bc+bMUWho1XZyOp1yOp3BLA0AALQAQZ1xcTgcio2NVX5+vn/M5/MpPz9fCQkJ1W5z9uzZKnESFhYmSTLGBLteAADQggV1xkWS0tLSlJycrLi4OA0ZMkRZWVkqKytTSkqKJGnatGnq2rWrMjMzJUnjxo3TsmXLNGjQIMXHx+vQoUN64oknNG7cOH/AAAAA1EbQ4ZKUlKRTp05p7ty5Kioq0sCBA5WXl+e/YPf48eMBZ1gef/xxhYSE6PHHH9cXX3yhTp06ady4cXr66afr71kAAIAWIcRY8H5NaWmpIiMjVVJSooiIiKZeDgAAqIWGeP3mt4oAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFijTuGSnZ2tmJgYhYeHKz4+Xrt3777o/NOnTys1NVWdO3eW0+lU7969lZubW6cFAwCAlqtVsBts3rxZaWlpWrVqleLj45WVlaXExEQdPHhQUVFRVeZXVFTopptuUlRUlF599VV17dpVx44dU7t27epj/QAAoAUJMcaYYDaIj4/X4MGDtWLFCkmSz+eT2+3Wgw8+qFmzZlWZv2rVKv35z3/WgQMH1Lp16zotsrS0VJGRkSopKVFERESdHgMAADSuhnj9DuqtooqKCu3Zs0cej+fHBwgNlcfjUUFBQbXbvP7660pISFBqaqpcLpf69++vhQsXqrKyssb9lJeXq7S0NOAGAAAQVLgUFxersrJSLpcrYNzlcqmoqKjabQ4fPqxXX31VlZWVys3N1RNPPKGlS5fqqaeeqnE/mZmZioyM9N/cbncwywQAAM1Ug3+qyOfzKSoqSqtXr1ZsbKySkpI0Z84crVq1qsZt0tPTVVJS4r+dOHGioZcJAAAsENTFuR07dlRYWJi8Xm/AuNfrVXR0dLXbdO7cWa1bt1ZYWJh/7JprrlFRUZEqKirkcDiqbON0OuV0OoNZGgAAaAGCOuPicDgUGxur/Px8/5jP51N+fr4SEhKq3WbYsGE6dOiQfD6ff+yTTz5R586dq40WAACAmgT9VlFaWprWrFmjl156Sfv379d9992nsrIypaSkSJKmTZum9PR0//z77rtPX3/9tR566CF98skn2rZtmxYuXKjU1NT6exYAAKBFCPp7XJKSknTq1CnNnTtXRUVFGjhwoPLy8vwX7B4/flyhoT/2kNvt1vbt2zVz5kxdf/316tq1qx566CE99thj9fcsAABAixD097g0Bb7HBQAA+zT597gAAAA0JcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDUIFwAAYA3CBQAAWINwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWqFO4ZGdnKyYmRuHh4YqPj9fu3btrtV1OTo5CQkI0YcKEuuwWAAC0cEGHy+bNm5WWlqaMjAzt3btXAwYMUGJiok6ePHnR7Y4ePaqHH35Yw4cPr/NiAQBAyxZ0uCxbtkx33XWXUlJSdO2112rVqlW64oortG7duhq3qays1G233ab58+erR48eP7uP8vJylZaWBtwAAACCCpeKigrt2bNHHo/nxwcIDZXH41FBQUGN2z355JOKiorSnXfeWav9ZGZmKjIy0n9zu93BLBMAADRTQYVLcXGxKisr5XK5AsZdLpeKioqq3WbXrl1au3at1qxZU+v9pKenq6SkxH87ceJEMMsEAADNVKuGfPAzZ85o6tSpWrNmjTp27Fjr7ZxOp5xOZwOuDAAA2CiocOnYsaPCwsLk9XoDxr1er6Kjo6vM/+yzz3T06FGNGzfOP+bz+b7fcatWOnjwoHr27FmXdQMAgBYoqLeKHA6HYmNjlZ+f7x/z+XzKz89XQkJClfl9+/bVhx9+qMLCQv9t/PjxGjlypAoLC7l2BQAABCXot4rS0tKUnJysuLg4DRkyRFlZWSorK1NKSookadq0aeratasyMzMVHh6u/v37B2zfrl07SaoyDgAA8HOCDpekpCSdOnVKc+fOVVFRkQYOHKi8vDz/BbvHjx9XaChfyAsAAOpfiDHGNPUifk5paakiIyNVUlKiiIiIpl4OAACohYZ4/ebUCAAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAa9QpXLKzsxUTE6Pw8HDFx8dr9+7dNc5ds2aNhg8frvbt26t9+/byeDwXnQ8AAFCToMNl8+bNSktLU0ZGhvbu3asBAwYoMTFRJ0+erHb+zp07NXnyZL399tsqKCiQ2+3W6NGj9cUXX1zy4gEAQMsSYowxwWwQHx+vwYMHa8WKFZIkn88nt9utBx98ULNmzfrZ7SsrK9W+fXutWLFC06ZNq3ZOeXm5ysvL/fdLS0vldrtVUlKiiIiIYJYLAACaSGlpqSIjI+v19TuoMy4VFRXas2ePPB7Pjw8QGiqPx6OCgoJaPcbZs2d1/vx5XXXVVTXOyczMVGRkpP/mdruDWSYAAGimggqX4uJiVVZWyuVyBYy7XC4VFRXV6jEee+wxdenSJSB+fio9PV0lJSX+24kTJ4JZJgAAaKZaNebOFi1apJycHO3cuVPh4eE1znM6nXI6nY24MgAAYIOgwqVjx44KCwuT1+sNGPd6vYqOjr7otkuWLNGiRYu0Y8cOXX/99cGvFAAAtHhBvVXkcDgUGxur/Px8/5jP51N+fr4SEhJq3G7x4sVasGCB8vLyFBcXV/fVAgCAFi3ot4rS0tKUnJysuLg4DRkyRFlZWSorK1NKSookadq0aeratasyMzMlSc8884zmzp2rTZs2KSYmxn8tTNu2bdW2bdt6fCoAAKC5CzpckpKSdOrUKc2dO1dFRUUaOHCg8vLy/BfsHj9+XKGhP57Ief7551VRUaFbb7014HEyMjI0b968S1s9AABoUYL+Hpem0BCfAwcAAA2ryb/HBQAAoCkRLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBqECwAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwBuECAACsQbgAAABrEC4AAMAahAsAALAG4QIAAKxBuAAAAGsQLgAAwBp1Cpfs7GzFxMQoPDxc8fHx2r1790Xnv/LKK+rbt6/Cw8N13XXXKTc3t06LBQAALVvQ4bJ582alpaUpIyNDe/fu1YABA5SYmKiTJ09WO//999/X5MmTdeedd2rfvn2aMGGCJkyYoI8++uiSFw8AAFqWEGOMCWaD+Ph4DR48WCtWrJAk+Xw+ud1uPfjgg5o1a1aV+UlJSSorK9Mbb7zhH/v1r3+tgQMHatWqVbXaZ2lpqSIjI1VSUqKIiIhglgsAAJpIQ7x+twpmckVFhfbs2aP09HT/WGhoqDwejwoKCqrdpqCgQGlpaQFjiYmJ2rp1a437KS8vV3l5uf9+SUmJpO//AgAAgB1+eN0O8hzJRQUVLsXFxaqsrJTL5QoYd7lcOnDgQLXbFBUVVTu/qKioxv1kZmZq/vz5VcbdbncwywUAAJeBr776SpGRkfXyWEGFS2NJT08POEtz+vRpdevWTcePH6+3J466KS0tldvt1okTJ3jbrolxLC4fHIvLC8fj8lFSUqKrr75aV111Vb09ZlDh0rFjR4WFhcnr9QaMe71eRUdHV7tNdHR0UPMlyel0yul0VhmPjIzkP8LLREREBMfiMsGxuHxwLC4vHI/LR2ho/X37SlCP5HA4FBsbq/z8fP+Yz+dTfn6+EhISqt0mISEhYL4kvfnmmzXOBwAAqEnQbxWlpaUpOTlZcXFxGjJkiLKyslRWVqaUlBRJ0rRp09S1a1dlZmZKkh566CGNGDFCS5cu1dixY5WTk6MPPvhAq1evrt9nAgAAmr2gwyUpKUmnTp3S3LlzVVRUpIEDByovL89/Ae7x48cDTgkNHTpUmzZt0uOPP67Zs2frV7/6lbZu3ar+/fvXep9Op1MZGRnVvn2ExsWxuHxwLC4fHIvLC8fj8tEQxyLo73EBAABoKvxWEQAAsAbhAgAArEG4AAAAaxAuAADAGoQLAACwxmUTLtnZ2YqJiVF4eLji4+O1e/fui85/5ZVX1LdvX4WHh+u6665Tbm5uI620+QvmWKxZs0bDhw9X+/bt1b59e3k8np89dqi9YP9d/CAnJ0chISGaMGFCwy6wBQn2WJw+fVqpqanq3LmznE6nevfuzf+n6kmwxyIrK0t9+vRRmzZt5Ha7NXPmTJ07d66RVtt8vfPOOxo3bpy6dOmikJCQi/548g927typG264QU6nU7169dKGDRuC37G5DOTk5BiHw2HWrVtn/vvf/5q77rrLtGvXzni93mrnv/feeyYsLMwsXrzYfPzxx+bxxx83rVu3Nh9++GEjr7z5CfZYTJkyxWRnZ5t9+/aZ/fv3mzvuuMNERkaazz//vJFX3vwEeyx+cOTIEdO1a1czfPhw87vf/a5xFtvMBXssysvLTVxcnBkzZozZtWuXOXLkiNm5c6cpLCxs5JU3P8Eei40bNxqn02k2btxojhw5YrZv3246d+5sZs6c2cgrb35yc3PNnDlzzGuvvWYkmS1btlx0/uHDh80VV1xh0tLSzMcff2yee+45ExYWZvLy8oLa72URLkOGDDGpqan++5WVlaZLly4mMzOz2vkTJ040Y8eODRiLj48399xzT4OusyUI9lj81IULF8yVV15pXnrppYZaYotRl2Nx4cIFM3ToUPPiiy+a5ORkwqWeBHssnn/+edOjRw9TUVHRWEtsMYI9FqmpqWbUqFEBY2lpaWbYsGENus6Wpjbh8uijj5p+/foFjCUlJZnExMSg9tXkbxVVVFRoz5498ng8/rHQ0FB5PB4VFBRUu01BQUHAfElKTEyscT5qpy7H4qfOnj2r8+fP1+svgbZEdT0WTz75pKKionTnnXc2xjJbhLoci9dff10JCQlKTU2Vy+VS//79tXDhQlVWVjbWspuluhyLoUOHas+ePf63kw4fPqzc3FyNGTOmUdaMH9XXa3fQX/lf34qLi1VZWen/yYAfuFwuHThwoNptioqKqp1fVFTUYOtsCepyLH7qscceU5cuXar8x4ng1OVY7Nq1S2vXrlVhYWEjrLDlqMuxOHz4sN566y3ddtttys3N1aFDh3T//ffr/PnzysjIaIxlN0t1ORZTpkxRcXGxbrzxRhljdOHCBd17772aPXt2YywZ/6Om1+7S0lJ99913atOmTa0ep8nPuKD5WLRokXJycrRlyxaFh4c39XJalDNnzmjq1Klas2aNOnbs2NTLafF8Pp+ioqK0evVqxcbGKikpSXPmzNGqVauaemktzs6dO7Vw4UKtXLlSe/fu1WuvvaZt27ZpwYIFTb001FGTn3Hp2LGjwsLC5PV6A8a9Xq+io6Or3SY6Ojqo+aiduhyLHyxZskSLFi3Sjh07dP311zfkMluEYI/FZ599pqNHj2rcuHH+MZ/PJ0lq1aqVDh48qJ49ezbsopupuvy76Ny5s1q3bq2wsDD/2DXXXKOioiJVVFTI4XA06Jqbq7ociyeeeEJTp07V9OnTJUnXXXedysrKdPfdd2vOnDkBPwqMhlXTa3dEREStz7ZIl8EZF4fDodjYWOXn5/vHfD6f8vPzlZCQUO02CQkJAfMl6c0336xxPmqnLsdCkhYvXqwFCxYoLy9PcXFxjbHUZi/YY9G3b199+OGHKiws9N/Gjx+vkSNHqrCwUG63uzGX36zU5d/FsGHDdOjQIX88StInn3yizp07Ey2XoC7H4uzZs1Xi5IegNPzGcKOqt9fu4K4bbhg5OTnG6XSaDRs2mI8//tjcfffdpl27dqaoqMgYY8zUqVPNrFmz/PPfe+8906pVK7NkyRKzf/9+k5GRwceh60mwx2LRokXG4XCYV1991Xz55Zf+25kzZ5rqKTQbwR6Ln+JTRfUn2GNx/Phxc+WVV5oHHnjAHDx40LzxxhsmKirKPPXUU031FJqNYI9FRkaGufLKK83f/vY3c/jwYfPPf/7T9OzZ00ycOLGpnkKzcebMGbNv3z6zb98+I8ksW7bM7Nu3zxw7dswYY8ysWbPM1KlT/fN/+Dj0I488Yvbv32+ys7Pt/Ti0McY899xz5uqrrzYOh8MMGTLE/Otf//L/2YgRI0xycnLA/Jdfftn07t3bOBwO069fP7Nt27ZGXnHzFcyx6Natm5FU5ZaRkdH4C2+Ggv138b8Il/oV7LF4//33TXx8vHE6naZHjx7m6aefNhcuXGjkVTdPwRyL8+fPm3nz5pmePXua8PBw43a7zf3332+++eabxl94M/P2229X+///H/7+k5OTzYgRI6psM3DgQONwOEyPHj3M+vXrg95viDGcKwMAAHZo8mtcAAAAaotwAQAA1iBcAACANQgXAABgDcIFAABYg3ABAADWIFwAAIA1CBcAAGANwgUAAFiDcAEAANYgXAAAgDX+H2lh6v/wwz4FAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Pooler - Tanh\")\n",
    "print(min(input_tanh))\n",
    "print(max(input_tanh))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c26a74f",
   "metadata": {},
   "source": [
    "encoder 1\n",
    "    1/x [1.34, 11546.02]\n",
    "    gelu [-14.58, 11.54]\n",
    "encoder 2\n",
    "    1/x [0.17, 14442144.63]\n",
    "    gelu [-17.57, 22.58]\n",
    "\n",
    "    tanh [-49.75, 42.18]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
